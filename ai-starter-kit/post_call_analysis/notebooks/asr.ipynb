{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import io\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import yaml\n",
    "import tarfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(os.path.join(repo_dir,'.env'))\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR With Diarization\n",
      "=====================================================\n",
      "Name                : ASR With Diarization\n",
      "ID                  : b6aefdf7-02a4-4384-9c3c-8a81d735a54e\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "ASR Without Diarization\n",
      "=====================================================\n",
      "Name                : ASR Without Diarization\n",
      "ID                  : a36cc322-dd36-40e3-9641-d87ac48fe2c4\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "CLIP\n",
      "=====================================================\n",
      "Name                : CLIP\n",
      "ID                  : 6c14325a-1be7-4e48-b38f-19b33745fc3b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Databox\n",
      "=====================================================\n",
      "Name                : Databox\n",
      "ID                  : 199e9684-785c-4df0-8dc3-49e808d8eba5\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Deepseek 6.7B single socket\n",
      "=====================================================\n",
      "Name                : Deepseek 6.7B single socket\n",
      "ID                  : 2eeb4b7f-bc56-48c4-8814-ef9d1e8806b8\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "DePlot\n",
      "=====================================================\n",
      "Name                : DePlot\n",
      "ID                  : 40f16b58-72a9-404f-a7c3-afc0d27a2343\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Dialog Act Classification\n",
      "=====================================================\n",
      "Name                : Dialog Act Classification\n",
      "ID                  : 0498c73a-5c03-456a-a645-3820728cfcae\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Document Classification\n",
      "=====================================================\n",
      "Name                : Document Classification\n",
      "ID                  : a28085cc-da42-44f7-9d06-95f60d06e4cb\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Fake Box for Testing\n",
      "=====================================================\n",
      "Name                : Fake Box for Testing\n",
      "ID                  : 3bcf5b6b-6d17-45ce-bb1e-543bed912f7b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "FFN MNIST\n",
      "=====================================================\n",
      "Name                : FFN MNIST\n",
      "ID                  : baf32143-cfee-4911-a5dd-545453e85b36\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Generative Tuning 1.5B\n",
      "=====================================================\n",
      "Name                : Generative Tuning 1.5B\n",
      "ID                  : e681c226-86be-40b2-9380-d2de11b19842\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Generative Tuning 13B\n",
      "=====================================================\n",
      "Name                : Generative Tuning 13B\n",
      "ID                  : 57f6a3c8-1f04-488a-bb39-3cfc5b4a5d7a\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Image Classification\n",
      "=====================================================\n",
      "Name                : Image Classification\n",
      "ID                  : 1c528476-f933-4168-b9fa-fb26a7180708\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 13B\n",
      "=====================================================\n",
      "Name                : Llama 2 13B\n",
      "ID                  : 1bf617cb-8afb-4bbd-b92f-c15ebfdca10b\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 70B\n",
      "=====================================================\n",
      "Name                : Llama 2 70B\n",
      "ID                  : 82254d3b-7239-458b-9da8-da1aca9b7fba\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 7B\n",
      "=====================================================\n",
      "Name                : Llama 2 7B\n",
      "ID                  : ec012370-6ffa-4a3a-b230-2c62613f1d89\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Llama 2 7B 8-socket\n",
      "=====================================================\n",
      "Name                : Llama 2 7B 8-socket\n",
      "ID                  : 21d706a3-d9fb-4998-aa9b-ad9ff2c3a920\n",
      "Playground          : True\n",
      "Prediction Input    : text\n",
      "\n",
      "Low Data Resource Text Classification\n",
      "=====================================================\n",
      "Name                : Low Data Resource Text Classification\n",
      "ID                  : 35f812a5-bb33-44e0-9e63-10191e44bb0c\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Named Entity Recognition\n",
      "=====================================================\n",
      "Name                : Named Entity Recognition\n",
      "ID                  : c27a105f-d0be-4bef-b2a4-4d6bf747ebdc\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Reranking\n",
      "=====================================================\n",
      "Name                : Reranking\n",
      "ID                  : f25c8247-f73a-4dd9-9871-d5c10675239c\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Sentence Detection\n",
      "=====================================================\n",
      "Name                : Sentence Detection\n",
      "ID                  : f67c5390-da52-4105-ae17-12434fa7d03b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Sentiment Analysis\n",
      "=====================================================\n",
      "Name                : Sentiment Analysis\n",
      "ID                  : 6db6d59f-ae40-4513-b218-4218fc97e40f\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Simple Text Classifier\n",
      "=====================================================\n",
      "Name                : Simple Text Classifier\n",
      "ID                  : 3aaf5b6b-6d17-45ce-bb1e-543bed912f7b\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Simple Text Classifier Pipeline\n",
      "=====================================================\n",
      "Name                : Simple Text Classifier Pipeline\n",
      "ID                  : cbc92c2f-2a01-4ef5-b85a-78ee7f392005\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n",
      "Speaker Diarization\n",
      "=====================================================\n",
      "Name                : Speaker Diarization\n",
      "ID                  : cbba6d31-104a-4295-ac21-7e91da09ab9b\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Speech Recognition\n",
      "=====================================================\n",
      "Name                : Speech Recognition\n",
      "ID                  : ecf84906-0924-4ce1-a1a2-c008f5334820\n",
      "Playground          : False\n",
      "Prediction Input    : file\n",
      "\n",
      "Text Embedding\n",
      "=====================================================\n",
      "Name                : Text Embedding\n",
      "ID                  : 89fbfbe6-ee77-4f5c-9ff6-56e2ab69f6ee\n",
      "Playground          : False\n",
      "Prediction Input    : text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!snapi app list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app id for ASR With Diarization\n",
    "\n",
    "app_id = 'b6aefdf7-02a4-4384-9c3c-8a81d735a54e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msnapi dataset add [OPTIONS]\u001b[0m\u001b[1m                                            \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " Add a new dataset                                                              \n",
      "                                                                                \n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-file\u001b[0m                          \u001b[1;33mTEXT\u001b[0m                                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-dataset\u001b[0m\u001b[1;36m-name\u001b[0m       \u001b[1;32m-n\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Dataset name \u001b[2m[default: None]\u001b[0m        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]  \u001b[0m                        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-apps\u001b[0m               \u001b[1;32m-apps\u001b[0m      \u001b[1;33mTEXT\u001b[0m  App IDs or names to which this      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Dataset will be associated          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-metadata\u001b[0m\u001b[1;36m-file\u001b[0m      \u001b[1;32m-mf\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Metadata File containing dataset    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          metadata file paths. Only           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          .yaml/.json files are supported.    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Example -                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          Dataset Metadata file schema        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"labels_file\":                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/labels_file\",              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"train_filepath\":                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/train_filepath\",           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"validation_filepath\":              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/validation_filepath\",      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"test_filepath\":                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"path/to/test_filepath\",            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          }                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-description\u001b[0m        \u001b[1;32m-d\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Free-form text description of       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-file_type\u001b[0m          \u001b[1;32m-ft\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Free-form text file types in        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-url\u001b[0m                \u001b[1;32m-u\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Free-form text url source of        \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          dataset                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-application_field\u001b[0m  \u001b[1;32m-af\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Field of application of dataset     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-language\u001b[0m           \u001b[1;32m-l\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Language of NLP dataset             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-job_type\u001b[0m           \u001b[1;32m-j\u001b[0m         \u001b[1;33mTEXT\u001b[0m  Job types for dataset (either       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          'train' & 'evaluation' or           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          'batch_predict')                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-source_type\u001b[0m        \u001b[1;32m-st\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Dataset Source type (either 'local' \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          & 'aws' or 'localMachine')          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m  \u001b[1;36m-\u001b[0m\u001b[1;36m-source_file\u001b[0m        \u001b[1;32m-sf\u001b[0m        \u001b[1;33mTEXT\u001b[0m  Source File. Only .yaml/.json files \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          are supported. source file should   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          contain one of source configs for   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          local, aws or localMachine          \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          1. Local Config                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {\"source_path\": \"string\"  // The    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          local source path on NFS.},         \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          2. AWS Config                       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"bucket\": \"string\",            //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          The AWS S3 bucket name.             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"folder\": \"string\",            //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          The folder or prefix within the     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          bucket.                             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"accessKeyId\": \"string\",       //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS Access Key ID for               \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          authentication.                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"secretAccessKey\": \"string\",   //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS Secret Access Key for           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          authentication.                     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \"region\": \"string\"             //   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          AWS region where the S3 bucket is   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          located.                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          },                                  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          3. Local Machine Config             \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          {\"source_path\": \"string\"  // The    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          source path on the local machine.}  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2m[default: None]                    \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                          \u001b[2;31m[required]                         \u001b[0m \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-test\u001b[0m               \u001b[1;32m-t\u001b[0m         \u001b[1;33m    \u001b[0m  Flag for test                       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                          \u001b[1;33m    \u001b[0m  Show this message and exit.         \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have to reference a json file that has the path of the data set that we'll upload\n",
    "\n",
    "!snapi dataset add --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(os.path.join(kit_dir,'config.yaml'))\n",
    "\n",
    "PENDING_RDU_JOB_STATUS = 'PENDING_RDU'\n",
    "SUCCESS_JOB_STATUS = 'EXIT_WITH_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchASRProcessor():\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        self.headers = {\n",
    "            'content-type': 'application/json',\n",
    "            'key': os.getenv('SAMBASTUDIO_KEY'),\n",
    "        }\n",
    "        self.datasets_path = f\".{config['asr']['datasets']['datasets_path']}\"\n",
    "        self.dataset_id = None\n",
    "        self.dataset_name = config['asr']['datasets']['dataset_name']\n",
    "        self.dataset_description = config['asr']['datasets']['dataset_description']\n",
    "        self.dataset_source_type = config['asr']['datasets']['dataset_source_type']\n",
    "        self.dataset_source_file = f\".{config['asr']['datasets']['dataset_source_file']}\"\n",
    "        self.dataset_language = config['asr']['datasets']['dataset_language']\n",
    "        \n",
    "        self.asr_with_diarization_app_id = config['asr']['apps']['asr_with_diarization_app_id']\n",
    "        self.application_field = config['asr']['apps']['application_field']\n",
    "        \n",
    "        self.base_url = config['asr']['urls']['base_url']\n",
    "        self.datasets_url = config['asr']['urls']['datasets_url'] \n",
    "        self.projects_url = config['asr']['urls']['projects_url'] \n",
    "        self.jobs_url = config['asr']['urls']['jobs_url'] \n",
    "        self.download_results_url = config['asr']['urls']['download_results_url'] \n",
    "    \n",
    "        self.project_name = config['asr']['projects']['project_name']\n",
    "        self.project_description = config['asr']['projects']['project_description']\n",
    "        self.project_id=None\n",
    "        \n",
    "        self.job_name = config['asr']['jobs']['job_name']\n",
    "        self.job_task = config['asr']['jobs']['job_task']\n",
    "        self.job_type = config['asr']['jobs']['job_type']\n",
    "        self.job_description = config['asr']['jobs']['job_description']\n",
    "        self.model_checkpoint = config['asr']['jobs']['model_checkpoint']\n",
    "        \n",
    "        self.output_path = config['asr']['output']['output_path']\n",
    "        \n",
    "        \n",
    "    def _get_call(self, url, params = None, success_message = None):\n",
    "        response = requests.get(url, params=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('GET request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'GET request failed with status code: {response.status_code}')\n",
    "            logging.error(f'Error message: {response.text}')\n",
    "        return response\n",
    "\n",
    "    def _post_call(self, url, params, success_message = None):\n",
    "        response = requests.post(url, json=params, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logging.info('POST request successful!')\n",
    "            logging.info(success_message)\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'POST request failed with status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')\n",
    "        return response\n",
    "    \n",
    "    def _delete_call(self, url):\n",
    "        response = requests.delete(url, headers=self.headers)    \n",
    "        if response.status_code == 200:\n",
    "            logging.info(f'Dataset {self.dataset_name} deleted successfully.')\n",
    "            logging.debug(f'Response: {response.text}')\n",
    "        else:\n",
    "            logging.error(f'Failed to delete the resource. Status code: {response.status_code}')\n",
    "            raise Exception(f'Error message: {response.text}')    \n",
    "        return response\n",
    "\n",
    "    def _time_to_seconds(self, time_str):\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        return  minutes * 60 + seconds\n",
    "\n",
    "    def _get_df_output(self, response_content: str) -> DataFrame:\n",
    "        compressed_bytes = io.BytesIO(response_content)\n",
    "        \n",
    "        with tarfile.open(fileobj=compressed_bytes, mode=\"r:gz\") as tar:\n",
    "            output_tar_member = tar.getmember(self.output_path)\n",
    "            output_file = tar.extractfile(output_tar_member)\n",
    "            output_df = pd.read_csv(io.BytesIO(output_file.read()), names = ['audio_path', 'results_path', 'speaker', 'start_time', 'sample_duration', 'unformatted_transcript', 'formatted_transcript'])\n",
    "            output_df['start_time'] = output_df.apply(lambda x: self._time_to_seconds(x['start_time']), axis = 1)\n",
    "            output_df['end_time'] = output_df.apply(lambda x: x['start_time'] + int(x['sample_duration'])/16000, axis = 1)\n",
    "            output_df = output_df[['start_time', 'end_time', 'speaker', 'formatted_transcript']].rename(columns={'formatted_transcript': 'text'})\n",
    "        \n",
    "        return output_df\n",
    "\n",
    "    def search_dataset(self, dataset_name):\n",
    "        url = self.base_url + self.datasets_url + '/search'\n",
    "        params = {\n",
    "            'dataset_name': dataset_name\n",
    "        }\n",
    "        response = self._get_call(url, params, f'Dataset {dataset_name} found in SambaStudio')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        return parsed_reponse['data']['dataset_id']\n",
    "\n",
    "    def delete_dataset(self, dataset_name):\n",
    "        dataset_id = self.search_dataset(dataset_name)\n",
    "        url = self.base_url + self.datasets_url + '/' + dataset_id\n",
    "        response = self._delete_call(url)\n",
    "        logging.info(response.text)\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, path):\n",
    "                \n",
    "        dataset_name = f'{self.dataset_name}_{int(time.time())}'\n",
    "        \n",
    "        # create pca directory and source.json file\n",
    "        pca_directory = self.datasets_path + '/' + dataset_name\n",
    "        \n",
    "        if not os.path.isdir(self.datasets_path):\n",
    "            os.mkdir(self.datasets_path) \n",
    "            \n",
    "        if not os.path.isdir(pca_directory):\n",
    "            logging.info(f'Datasets path: {pca_directory} wan\\'t found')\n",
    "            \n",
    "            source_file_data = {\n",
    "                \"source_path\": pca_directory\n",
    "            }\n",
    "            with open(self.dataset_source_file, 'w') as json_file:\n",
    "                json.dump(source_file_data, json_file)\n",
    "            os.mkdir(pca_directory)\n",
    "            \n",
    "            logging.info(f'PCA Directory: {pca_directory} created')\n",
    "    \n",
    "        # validate audio file\n",
    "        audio_format = path.split('.')[-1]\n",
    "        \n",
    "        if audio_format == 'mp3':\n",
    "            shutil.copyfile(path, pca_directory + '/pca_file.mp3')\n",
    "        elif audio_format == 'wav':\n",
    "            shutil.copyfile(path, pca_directory + '/pca_file.wav')\n",
    "        else:\n",
    "            raise Exception('Only mp3 and wav audio files supported')\n",
    "        \n",
    "        # create dataset\n",
    "        command = f'echo yes | snapi dataset add \\\n",
    "            --dataset-name {dataset_name} \\\n",
    "            --job_type {self.job_type} \\\n",
    "            --apps {self.asr_with_diarization_app_id} \\\n",
    "            --source_type {self.dataset_source_type} \\\n",
    "            --source_file {self.dataset_source_file} \\\n",
    "            --application_field {self.application_field} \\\n",
    "            --language {self.dataset_language} \\\n",
    "            --description \"{self.dataset_description}\"'\n",
    "        \n",
    "        os.system(command)\n",
    "        logging.info(f'Creating dataset: {dataset_name}')\n",
    "        \n",
    "        return dataset_name\n",
    "                \n",
    "    def check_dataset_creation_progress(self, dataset_name):\n",
    "        url = self.base_url + self.datasets_url + '/' + dataset_name\n",
    "        response = self._get_call(url)\n",
    "        if response.json()[\"data\"][\"status\"]==\"Available\": \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def create_load_project(self):\n",
    "\n",
    "        url = self.base_url + self.projects_url + '/' + self.project_name\n",
    "\n",
    "        response = self._get_call(url, success_message=f'Project {self.project_name} found in SambaStudio')\n",
    "        not_found_error_message = f\"{self.project_name} not found\"\n",
    "\n",
    "        if not_found_error_message in response.text:\n",
    "            \n",
    "            logging.info(f'Project {self.project_name} wasn\\'t found in SambaStudio')\n",
    "            \n",
    "            url = self.base_url + self.projects_url\n",
    "\n",
    "            params = {\n",
    "                'project_name': self.project_name,\n",
    "                'description': self.project_description\n",
    "            }\n",
    "\n",
    "            response = self._post_call(url, params, success_message=f'Project {self.project_name} created!')\n",
    "\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        self.project_id = parsed_reponse['data']['project_id']\n",
    "        return self.project_id\n",
    "    \n",
    "    def run_job(self, dataset_name):\n",
    "        \n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id)\n",
    "        \n",
    "        params = {\n",
    "            'task': self.job_task,\n",
    "            'job_type': self.job_type,\n",
    "            'job_name': f'{self.job_name}_{int(time.time())}',\n",
    "            'project': self.project_id,\n",
    "            'model_checkpoint': self.model_checkpoint,\n",
    "            'description': self.job_description,\n",
    "            'dataset': dataset_name,\n",
    "        }\n",
    "\n",
    "        response = self._post_call(url, params, success_message='Job running')\n",
    "        parsed_reponse = json.loads(response.text)\n",
    "        job_id = parsed_reponse['data']['job_id']\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def check_job_progress(self, job_id):\n",
    "\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id\n",
    "\n",
    "        status = PENDING_RDU_JOB_STATUS\n",
    "        while status != SUCCESS_JOB_STATUS:\n",
    "            response = self._get_call(url, success_message='Still waiting for job to finish')\n",
    "            parsed_reponse = json.loads(response.text)   \n",
    "            status = parsed_reponse['data']['status']\n",
    "            logging.info(f'Job status: {status}')\n",
    "            if status == SUCCESS_JOB_STATUS:\n",
    "                logging.info('Job finished!')\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def delete_job(self, job_id):\n",
    "        url = self.base_url +  self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id\n",
    "        response = self._delete_call(url)\n",
    "        logging.info(response.text)\n",
    "        \n",
    "    def retrieve_results(self, job_id):\n",
    "        url = self.base_url + self.projects_url + self.jobs_url.format(project_id=self.project_id) + '/' + job_id + self.download_results_url\n",
    "        response = self._get_call(url, success_message='Results downloaded!')\n",
    "        df = self._get_df_output(response.content)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = BatchASRProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Project PCA_Project found in SambaStudio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2d49f0cd-d807-488b-a527-c2dd43377dd9'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr.create_load_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Datasets path: ../data/datasets/PCA_dataset_1709326005 wan't found\n",
      "INFO:root:PCA Directory: ../data/datasets/PCA_dataset_1709326005 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder Information:\n",
      "  - Number of Files: 1\n",
      "  - Total Size: 7.12 MB\n",
      "\n",
      "Are you sure you want to proceed? (\u001b[33myes\u001b[0m/no)\n",
      ": Uploading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating dataset: PCA_dataset_1709326005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset folder upload complete: ../data/datasets/PCA_dataset_1709326005\n",
      "Dataset added successfully.\n",
      "Time taken to upload the dataset: 3.8197240829467773 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:None\n"
     ]
    }
   ],
   "source": [
    "dataset_name = asr.create_dataset(path=os.path.join(kit_dir,'data/conversations/audio/911_call.wav'))\n",
    "while not asr.check_dataset_creation_progress(dataset_name):\n",
    "    print(\"waiting for dataset available\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Dataset PCA_dataset_1709326005 found in SambaStudio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'b125ec06-4c05-4960-901d-a811a6e01954'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr.search_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:POST request successful!\n",
      "INFO:root:Job running\n"
     ]
    }
   ],
   "source": [
    "job_id = asr.run_job(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PENDING_RDU\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: PREDICTING\n",
      "INFO:root:GET request successful!\n",
      "INFO:root:Still waiting for job to finish\n",
      "INFO:root:Job status: EXIT_WITH_0\n",
      "INFO:root:Job finished!\n"
     ]
    }
   ],
   "source": [
    "result = asr.check_job_progress(job_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Results downloaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Our Primeti 33. What is yet as emergency?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>11.9</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, sir, I need to, uh, uh. I need an ambulan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>A car.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>14.7</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Carol Wood Drive. Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>14.6</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>16.6</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yeah, a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>21.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, sir. What's the phone number you calling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>31.6</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, sir. Oh, I have a we have a a gentleman he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>37.2</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, how does. He's a 50 years old. Ser 50. O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38</td>\n",
       "      <td>39.5</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, he's not breathing, sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40</td>\n",
       "      <td>53.0</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, and he's not conscious either. Don't con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56</td>\n",
       "      <td>58.0</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>We need them. You get when you just party on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>57</td>\n",
       "      <td>62.9</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Just you on way there? We're on Wemo. But I ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>63</td>\n",
       "      <td>71.9</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Yes, we have a personal doctor if you're with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72</td>\n",
       "      <td>82.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Oh, okay, well, we're on our way there. If you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84</td>\n",
       "      <td>87.3</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, no, just a doctor. So the doctor's been th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>87</td>\n",
       "      <td>88.7</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, so did doctor see what happen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>89</td>\n",
       "      <td>96.7</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Uh, um, Doctor, did you see what happens there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96</td>\n",
       "      <td>100.4</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, we're in a way. I'm just I'm just passin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101</td>\n",
       "      <td>105.5</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>A your, uh. he's pumping. He's pumping his che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>105</td>\n",
       "      <td>114.6</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Okay, okay, we on. We les than a mile away. Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_time  end_time     speaker  \\\n",
       "0            0       2.5  SPEAKER_01   \n",
       "1            3      11.9  SPEAKER_00   \n",
       "2           11      12.3  SPEAKER_01   \n",
       "3           13      14.7  SPEAKER_00   \n",
       "4           14      14.6  SPEAKER_01   \n",
       "5           15      16.6  SPEAKER_00   \n",
       "6           17      21.4  SPEAKER_01   \n",
       "7           22      31.6  SPEAKER_00   \n",
       "8           32      37.2  SPEAKER_01   \n",
       "9           38      39.5  SPEAKER_00   \n",
       "10          40      53.0  SPEAKER_01   \n",
       "11          56      58.0  SPEAKER_00   \n",
       "12          57      62.9  SPEAKER_01   \n",
       "13          63      71.9  SPEAKER_00   \n",
       "14          72      82.4  SPEAKER_01   \n",
       "15          84      87.3  SPEAKER_00   \n",
       "16          87      88.7  SPEAKER_01   \n",
       "17          89      96.7  SPEAKER_00   \n",
       "18          96     100.4  SPEAKER_01   \n",
       "19         101     105.5  SPEAKER_00   \n",
       "20         105     114.6  SPEAKER_01   \n",
       "\n",
       "                                                 text  \n",
       "0           Our Primeti 33. What is yet as emergency?  \n",
       "1   Yes, sir, I need to, uh, uh. I need an ambulan...  \n",
       "2                                              A car.  \n",
       "3                              Carol Wood Drive. Yes.  \n",
       "4                                                 NaN  \n",
       "5                                            Yeah, a.  \n",
       "6   Okay, sir. What's the phone number you calling...  \n",
       "7   Uh, sir. Oh, I have a we have a a gentleman he...  \n",
       "8   Okay, how does. He's a 50 years old. Ser 50. O...  \n",
       "9                       Yes, he's not breathing, sir.  \n",
       "10  Okay, and he's not conscious either. Don't con...  \n",
       "11      We need them. You get when you just party on.  \n",
       "12  Just you on way there? We're on Wemo. But I ki...  \n",
       "13  Yes, we have a personal doctor if you're with ...  \n",
       "14  Oh, okay, well, we're on our way there. If you...  \n",
       "15  Uh, no, just a doctor. So the doctor's been th...  \n",
       "16               Okay, so did doctor see what happen.  \n",
       "17  Uh, um, Doctor, did you see what happens there...  \n",
       "18  Okay, we're in a way. I'm just I'm just passin...  \n",
       "19  A your, uh. he's pumping. He's pumping his che...  \n",
       "20  Okay, okay, we on. We les than a mile away. Th...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = asr.retrieve_results(job_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset PCA_dataset deleted successfully.\n",
      "INFO:root:{}\n"
     ]
    }
   ],
   "source": [
    "asr.delete_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GET request successful!\n",
      "INFO:root:Dataset PCA_dataset_1709326005 found in SambaStudio\n",
      "INFO:root:Dataset PCA_dataset deleted successfully.\n",
      "INFO:root:{\"detail\":\"The Dataset: b125ec06-4c05-4960-901d-a811a6e01954 was successfully marked for deletion from the Dataset Hub.\"}\n"
     ]
    }
   ],
   "source": [
    "asr.delete_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
