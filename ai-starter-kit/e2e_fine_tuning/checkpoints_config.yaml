sambastudio:
  snapi_path: ""
  rdu_arch: "SN40L-8"

checkpoints:
  - model_name: "Suzume-Llama-3-8B-Multilingual"
    checkpoint_path: '/Users/my_user/Documents/ai-starter-kit/e2e_fine_tuning/data/models/models--lightblue--suzume-llama-3-8B-multilingual/snapshots/0cb15aa9ec685eef494f9a15f65aefcfe3c04c66'
    publisher: "lightblue"
    description: "Suzume 8B, a multilingual finetune of Llama "
    param_count: 8
    model_arch: "llama" 
    seq_length: 8192 
    vocab_size: 128256 
    app_id: "61fa0993-04a2-42ca-9db1-1eff693ea978"

dataset:
  dataset_name: "publichealth"
  dataset_description: "This dataset contains question and answer pairs sourced from Q&A pages and FAQs from CDC and WHO pertaining to COVID-19"
  dataset_path: "/Users/my_user/Documents/ai-starter-kit/e2e_fine_tuning/data/datasets/fine_tuning-publichealth-qa"
  dataset_apps_availability: 
    - 'Llama 3'
    - 'Samba1 Llama3 Experts'
    - 'Samba1 Llama3.1 Experts'
    - 'Samba1 Llama3.2 Experts'
  dataset_job_types:
    - "evaluation"
    - "train"
  dataset_source_type: "localMachine"
  dataset_language: "english"
  dataset_filetype: "hdf5"
  dataset_url: "https://huggingface.co/datasets/xhluca/publichealth-qa"
  dataset_metadata:
    labels_file: ""
    train_filepath: ""
    validation_filepath: ""
    test_filepath: ""

project:
  project_name: "byoc fine-tuning project"
  project_description: "this project will be used to test the BYOC and Fine-tuning e2e pipeline implementation"

job:
  job_name: "e2e_fc_taining_job"
  job_description: "e2e finetuning training job public health for suzume multilingual"
  job_type: "train"
  model: "'Suzume-Llama-3-8B-Multilingual"
  model_version: "1"
  dataset_name: 'publichealth'
  parallel_instances: 1
  load_state: false
  sub_path: ""
  hyperparams:
    batch_size: 8
    do_eval: false
    eval_steps: 50
    evaluation_strategy: "no"
    learning_rate: 0.00001
    logging_steps: 1
    lr_schedule: "fixed_lr"
    max_sequence_length: 8192
    num_iterations: 100
    prompt_loss_weight: 0.0
    save_optimizer_state: True
    save_steps: 50
    skip_checkpoint: False
    subsample_eval: 0.01
    subsample_eval_seed: 123
    use_token_type_ids: true
    vocab_size: 128256
    warmup_steps: 0
    weight_decay: 0.1

model_checkpoint: # for promotion, deletion, and deployment 
  checkpoint_name: "" #set after listing the generated checkpoints after training
  model_name: "llama2_7b_fine_tuned"
  model_version: "1"
  model_description: "finetuned llama2_7b model"
  model_type: "finetuned"